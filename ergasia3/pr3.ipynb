{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stathis/miniconda3/envs/AI2/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/stathis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "inputFile = \"../ergasia_1/imdb-reviews.csv\"\n",
    "\n",
    "# Insert the path of the file you want to test HERE\n",
    "testFile = None\n",
    "\n",
    "df = pd.read_csv(inputFile, sep='\\t')\n",
    "\n",
    "# Drop the first column (url)\n",
    "df = df.drop(columns=['url'])\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # if score is greater than 7, it is positive so change its value to 1, else 0\n",
    "    if row[\"rating\"] >= 7:\n",
    "        positive += 1\n",
    "        df.at[index, \"rating\"] = 1\n",
    "    elif row[\"rating\"] <= 4:\n",
    "        negative += 1\n",
    "        df.at[index, \"rating\"] = 0\n",
    "    else:\n",
    "        neutral += 1\n",
    "    # To Remove the stop words uncomment the below line. Does not improve accuracy\n",
    "    # df.at[index, \"review\"] = ' '.join([word for word in row[\"review\"].split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get the GloVe word embeddings\n",
    "gloveFile = \"../glove.twitter.27B.200d.txt\"\n",
    "\n",
    "embeddings = {}\n",
    "with open(gloveFile, 'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size:  (45008, 2)\n",
      "train size:  (36006, 2)\n",
      "test size:  (4501, 2)\n",
      "validation size:  (4501, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train 80% , test 10% and validation 10% sets\n",
    "\n",
    "train = df.sample(frac=0.8, random_state=42)\n",
    "test = df.drop(train.index)\n",
    "test = test.sample(frac=0.5, random_state=42)\n",
    "validation = df.drop(train.index)\n",
    "validation = validation.drop(test.index)\n",
    "\n",
    "if testFile is not None:\n",
    "    test = pd.read_csv(testFile, sep='\\t')\n",
    "    test = test.drop(columns=['url'])\n",
    "\n",
    "# print df, train, test, validation sizes\n",
    "print(\"df size: \", df.shape)\n",
    "print(\"train size: \", train.shape)\n",
    "print(\"test size: \", test.shape)\n",
    "print(\"validation size: \", validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X_train, y_train, X_test, y_test, X_validation, y_validation\n",
    "X_train = train[\"review\"]\n",
    "y_train = train[\"rating\"]\n",
    "X_test = test[\"review\"]\n",
    "y_test = test[\"rating\"]\n",
    "X_validation = validation[\"review\"]\n",
    "y_validation = validation[\"rating\"]\n",
    "\n",
    "\n",
    "# Create word vectors for train, test and validation sets\n",
    "def createWordVectors(data):\n",
    "  wordVectors = [];\n",
    "  for row in data:\n",
    "    sum = np.zeros(len(embeddings[\"hello\"]));\n",
    "    word_count = 0;\n",
    "    for word in row.split():\n",
    "      if word in embeddings:\n",
    "        sum = sum + embeddings[word];\n",
    "        word_count = word_count+1;\n",
    "    if word_count != 0:\n",
    "      wordVectors.append(sum/word_count);\n",
    "    else:\n",
    "      wordVectors.append(np.zeros(len(embeddings[\"hello\"])))\n",
    "\n",
    "  wordVectors = np.array(wordVectors, dtype=np.float32)\n",
    "  return wordVectors\n",
    "\n",
    "\n",
    "X_train = createWordVectors(X_train)\n",
    "X_test = createWordVectors(X_test)\n",
    "X_validation = createWordVectors(X_validation)\n",
    "\n",
    "# Convert the word vectors from ndarrays to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "X_validation = torch.tensor(X_validation, dtype=torch.float)\n",
    "\n",
    "# Convert the labels to tensors\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "y_validation = torch.tensor(y_validation.values, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/36006], Loss: 0.6942\n",
      "Epoch [1/10], Step [200/36006], Loss: 0.6281\n",
      "Epoch [1/10], Step [300/36006], Loss: 0.5907\n",
      "Epoch [1/10], Step [400/36006], Loss: 0.9674\n",
      "Epoch [1/10], Step [500/36006], Loss: 0.7307\n",
      "Epoch [1/10], Step [600/36006], Loss: 0.4721\n",
      "Epoch [1/10], Step [700/36006], Loss: 1.3581\n",
      "Epoch [1/10], Step [800/36006], Loss: 0.6551\n",
      "Epoch [1/10], Step [900/36006], Loss: 0.8071\n",
      "Epoch [1/10], Step [1000/36006], Loss: 1.2123\n",
      "Epoch [1/10], Step [1100/36006], Loss: 0.1029\n",
      "Epoch [1/10], Step [1200/36006], Loss: 0.7776\n",
      "Epoch [1/10], Step [1300/36006], Loss: 0.3442\n",
      "Epoch [1/10], Step [1400/36006], Loss: 0.4274\n",
      "Epoch [1/10], Step [1500/36006], Loss: 0.2070\n",
      "Epoch [1/10], Step [1600/36006], Loss: 0.6865\n",
      "Epoch [1/10], Step [1700/36006], Loss: 0.6760\n",
      "Epoch [1/10], Step [1800/36006], Loss: 0.2190\n",
      "Epoch [1/10], Step [1900/36006], Loss: 0.2420\n",
      "Epoch [1/10], Step [2000/36006], Loss: 0.1146\n",
      "Epoch [1/10], Step [2100/36006], Loss: 0.1593\n",
      "Epoch [1/10], Step [2200/36006], Loss: 1.6332\n",
      "Epoch [1/10], Step [2300/36006], Loss: 0.0387\n",
      "Epoch [1/10], Step [2400/36006], Loss: 1.4609\n",
      "Epoch [1/10], Step [2500/36006], Loss: 0.1124\n",
      "Epoch [1/10], Step [2600/36006], Loss: 0.0411\n",
      "Epoch [1/10], Step [2700/36006], Loss: 0.8234\n",
      "Epoch [1/10], Step [2800/36006], Loss: 0.0677\n",
      "Epoch [1/10], Step [2900/36006], Loss: 0.2823\n",
      "Epoch [1/10], Step [3000/36006], Loss: 1.3985\n",
      "Epoch [1/10], Step [3100/36006], Loss: 0.0752\n",
      "Epoch [1/10], Step [3200/36006], Loss: 0.0551\n",
      "Epoch [1/10], Step [3300/36006], Loss: 0.0577\n",
      "Epoch [1/10], Step [3400/36006], Loss: 0.1040\n",
      "Epoch [1/10], Step [3500/36006], Loss: 0.5927\n",
      "Epoch [1/10], Step [3600/36006], Loss: 0.6519\n",
      "Epoch [1/10], Step [3700/36006], Loss: 0.1488\n",
      "Epoch [1/10], Step [3800/36006], Loss: 0.5094\n",
      "Epoch [1/10], Step [3900/36006], Loss: 0.1071\n",
      "Epoch [1/10], Step [4000/36006], Loss: 0.0884\n",
      "Epoch [1/10], Step [4100/36006], Loss: 0.0947\n",
      "Epoch [1/10], Step [4200/36006], Loss: 0.1543\n",
      "Epoch [1/10], Step [4300/36006], Loss: 0.2693\n",
      "Epoch [1/10], Step [4400/36006], Loss: 0.1220\n",
      "Epoch [1/10], Step [4500/36006], Loss: 0.1247\n",
      "Epoch [1/10], Step [4600/36006], Loss: 0.1351\n",
      "Epoch [1/10], Step [4700/36006], Loss: 0.3684\n",
      "Epoch [1/10], Step [4800/36006], Loss: 0.2860\n",
      "Epoch [1/10], Step [4900/36006], Loss: 0.3497\n",
      "Epoch [1/10], Step [5000/36006], Loss: 0.7860\n",
      "Epoch [1/10], Step [5100/36006], Loss: 0.1455\n",
      "Epoch [1/10], Step [5200/36006], Loss: 1.2970\n",
      "Epoch [1/10], Step [5300/36006], Loss: 1.6297\n",
      "Epoch [1/10], Step [5400/36006], Loss: 0.2328\n",
      "Epoch [1/10], Step [5500/36006], Loss: 0.2523\n",
      "Epoch [1/10], Step [5600/36006], Loss: 0.1089\n",
      "Epoch [1/10], Step [5700/36006], Loss: 0.2114\n",
      "Epoch [1/10], Step [5800/36006], Loss: 0.0860\n",
      "Epoch [1/10], Step [5900/36006], Loss: 0.0459\n",
      "Epoch [1/10], Step [6000/36006], Loss: 0.6228\n",
      "Epoch [1/10], Step [6100/36006], Loss: 0.0365\n",
      "Epoch [1/10], Step [6200/36006], Loss: 0.1376\n",
      "Epoch [1/10], Step [6300/36006], Loss: 0.3781\n",
      "Epoch [1/10], Step [6400/36006], Loss: 0.1352\n",
      "Epoch [1/10], Step [6500/36006], Loss: 0.6084\n",
      "Epoch [1/10], Step [6600/36006], Loss: 0.0507\n",
      "Epoch [1/10], Step [6700/36006], Loss: 0.1893\n",
      "Epoch [1/10], Step [6800/36006], Loss: 0.0269\n",
      "Epoch [1/10], Step [6900/36006], Loss: 0.7204\n",
      "Epoch [1/10], Step [7000/36006], Loss: 0.1115\n",
      "Epoch [1/10], Step [7100/36006], Loss: 1.6587\n",
      "Epoch [1/10], Step [7200/36006], Loss: 0.3117\n",
      "Epoch [1/10], Step [7300/36006], Loss: 0.0092\n",
      "Epoch [1/10], Step [7400/36006], Loss: 0.0710\n",
      "Epoch [1/10], Step [7500/36006], Loss: 0.2864\n",
      "Epoch [1/10], Step [7600/36006], Loss: 2.4897\n",
      "Epoch [1/10], Step [7700/36006], Loss: 0.0372\n",
      "Epoch [1/10], Step [7800/36006], Loss: 0.0359\n",
      "Epoch [1/10], Step [7900/36006], Loss: 0.8361\n",
      "Epoch [1/10], Step [8000/36006], Loss: 2.6343\n",
      "Epoch [1/10], Step [8100/36006], Loss: 0.0448\n",
      "Epoch [1/10], Step [8200/36006], Loss: 0.0212\n",
      "Epoch [1/10], Step [8300/36006], Loss: 0.0424\n",
      "Epoch [1/10], Step [8400/36006], Loss: 0.0360\n",
      "Epoch [1/10], Step [8500/36006], Loss: 0.0272\n",
      "Epoch [1/10], Step [8600/36006], Loss: 0.2278\n",
      "Epoch [1/10], Step [8700/36006], Loss: 0.0704\n",
      "Epoch [1/10], Step [8800/36006], Loss: 1.1985\n",
      "Epoch [1/10], Step [8900/36006], Loss: 1.0366\n",
      "Epoch [1/10], Step [9000/36006], Loss: 0.1294\n",
      "Epoch [1/10], Step [9100/36006], Loss: 0.3580\n",
      "Epoch [1/10], Step [9200/36006], Loss: 0.3859\n",
      "Epoch [1/10], Step [9300/36006], Loss: 0.9492\n",
      "Epoch [1/10], Step [9400/36006], Loss: 0.6545\n",
      "Epoch [1/10], Step [9500/36006], Loss: 0.1048\n",
      "Epoch [1/10], Step [9600/36006], Loss: 0.0674\n",
      "Epoch [1/10], Step [9700/36006], Loss: 1.5345\n",
      "Epoch [1/10], Step [9800/36006], Loss: 0.0362\n",
      "Epoch [1/10], Step [9900/36006], Loss: 0.1921\n",
      "Epoch [1/10], Step [10000/36006], Loss: 0.5103\n",
      "Epoch [1/10], Step [10100/36006], Loss: 0.1686\n",
      "Epoch [1/10], Step [10200/36006], Loss: 0.8471\n",
      "Epoch [1/10], Step [10300/36006], Loss: 0.0443\n",
      "Epoch [1/10], Step [10400/36006], Loss: 0.7030\n",
      "Epoch [1/10], Step [10500/36006], Loss: 0.1947\n",
      "Epoch [1/10], Step [10600/36006], Loss: 0.7398\n",
      "Epoch [1/10], Step [10700/36006], Loss: 0.1262\n",
      "Epoch [1/10], Step [10800/36006], Loss: 0.1628\n",
      "Epoch [1/10], Step [10900/36006], Loss: 0.0193\n",
      "Epoch [1/10], Step [11000/36006], Loss: 0.1773\n",
      "Epoch [1/10], Step [11100/36006], Loss: 0.0256\n",
      "Epoch [1/10], Step [11200/36006], Loss: 0.7385\n",
      "Epoch [1/10], Step [11300/36006], Loss: 0.0319\n",
      "Epoch [1/10], Step [11400/36006], Loss: 0.2109\n",
      "Epoch [1/10], Step [11500/36006], Loss: 0.0433\n",
      "Epoch [1/10], Step [11600/36006], Loss: 0.1069\n",
      "Epoch [1/10], Step [11700/36006], Loss: 0.2509\n",
      "Epoch [1/10], Step [11800/36006], Loss: 0.1283\n",
      "Epoch [1/10], Step [11900/36006], Loss: 1.7491\n",
      "Epoch [1/10], Step [12000/36006], Loss: 0.0582\n",
      "Epoch [1/10], Step [12100/36006], Loss: 0.1484\n",
      "Epoch [1/10], Step [12200/36006], Loss: 2.8548\n",
      "Epoch [1/10], Step [12300/36006], Loss: 0.2722\n",
      "Epoch [1/10], Step [12400/36006], Loss: 0.1861\n",
      "Epoch [1/10], Step [12500/36006], Loss: 1.5948\n",
      "Epoch [1/10], Step [12600/36006], Loss: 0.7437\n",
      "Epoch [1/10], Step [12700/36006], Loss: 0.2825\n",
      "Epoch [1/10], Step [12800/36006], Loss: 0.5672\n",
      "Epoch [1/10], Step [12900/36006], Loss: 1.0295\n",
      "Epoch [1/10], Step [13000/36006], Loss: 0.1618\n",
      "Epoch [1/10], Step [13100/36006], Loss: 0.1943\n",
      "Epoch [1/10], Step [13200/36006], Loss: 0.4021\n",
      "Epoch [1/10], Step [13300/36006], Loss: 0.2061\n",
      "Epoch [1/10], Step [13400/36006], Loss: 0.0831\n",
      "Epoch [1/10], Step [13500/36006], Loss: 0.0465\n",
      "Epoch [1/10], Step [13600/36006], Loss: 1.5029\n",
      "Epoch [1/10], Step [13700/36006], Loss: 0.2244\n",
      "Epoch [1/10], Step [13800/36006], Loss: 0.1564\n",
      "Epoch [1/10], Step [13900/36006], Loss: 0.1302\n",
      "Epoch [1/10], Step [14000/36006], Loss: 0.1353\n",
      "Epoch [1/10], Step [14100/36006], Loss: 0.5783\n",
      "Epoch [1/10], Step [14200/36006], Loss: 0.4339\n",
      "Epoch [1/10], Step [14300/36006], Loss: 0.0265\n",
      "Epoch [1/10], Step [14400/36006], Loss: 1.1379\n",
      "Epoch [1/10], Step [14500/36006], Loss: 0.1991\n",
      "Epoch [1/10], Step [14600/36006], Loss: 0.0769\n",
      "Epoch [1/10], Step [14700/36006], Loss: 0.5658\n",
      "Epoch [1/10], Step [14800/36006], Loss: 0.2530\n",
      "Epoch [1/10], Step [14900/36006], Loss: 0.1845\n",
      "Epoch [1/10], Step [15000/36006], Loss: 0.4420\n",
      "Epoch [1/10], Step [15100/36006], Loss: 0.8517\n",
      "Epoch [1/10], Step [15200/36006], Loss: 1.1015\n",
      "Epoch [1/10], Step [15300/36006], Loss: 1.1262\n",
      "Epoch [1/10], Step [15400/36006], Loss: 0.0412\n",
      "Epoch [1/10], Step [15500/36006], Loss: 0.2265\n",
      "Epoch [1/10], Step [15600/36006], Loss: 0.4084\n",
      "Epoch [1/10], Step [15700/36006], Loss: 0.5851\n",
      "Epoch [1/10], Step [15800/36006], Loss: 0.0458\n",
      "Epoch [1/10], Step [15900/36006], Loss: 0.0267\n",
      "Epoch [1/10], Step [16000/36006], Loss: 0.5992\n",
      "Epoch [1/10], Step [16100/36006], Loss: 2.0038\n",
      "Epoch [1/10], Step [16200/36006], Loss: 0.7753\n",
      "Epoch [1/10], Step [16300/36006], Loss: 0.5998\n",
      "Epoch [1/10], Step [16400/36006], Loss: 0.2157\n",
      "Epoch [1/10], Step [16500/36006], Loss: 0.4671\n",
      "Epoch [1/10], Step [16600/36006], Loss: 0.2004\n",
      "Epoch [1/10], Step [16700/36006], Loss: 0.4941\n",
      "Epoch [1/10], Step [16800/36006], Loss: 0.0514\n",
      "Epoch [1/10], Step [16900/36006], Loss: 0.1321\n",
      "Epoch [1/10], Step [17000/36006], Loss: 0.2443\n",
      "Epoch [1/10], Step [17100/36006], Loss: 0.0386\n",
      "Epoch [1/10], Step [17200/36006], Loss: 0.8727\n",
      "Epoch [1/10], Step [17300/36006], Loss: 0.7153\n",
      "Epoch [1/10], Step [17400/36006], Loss: 0.9581\n",
      "Epoch [1/10], Step [17500/36006], Loss: 1.3928\n",
      "Epoch [1/10], Step [17600/36006], Loss: 1.4440\n",
      "Epoch [1/10], Step [17700/36006], Loss: 0.4048\n",
      "Epoch [1/10], Step [17800/36006], Loss: 0.0688\n",
      "Epoch [1/10], Step [17900/36006], Loss: 0.2209\n",
      "Epoch [1/10], Step [18000/36006], Loss: 0.0134\n",
      "Epoch [1/10], Step [18100/36006], Loss: 0.1663\n",
      "Epoch [1/10], Step [18200/36006], Loss: 0.0163\n",
      "Epoch [1/10], Step [18300/36006], Loss: 0.0530\n",
      "Epoch [1/10], Step [18400/36006], Loss: 1.0824\n",
      "Epoch [1/10], Step [18500/36006], Loss: 0.2041\n",
      "Epoch [1/10], Step [18600/36006], Loss: 0.2922\n",
      "Epoch [1/10], Step [18700/36006], Loss: 0.5303\n",
      "Epoch [1/10], Step [18800/36006], Loss: 0.0566\n",
      "Epoch [1/10], Step [18900/36006], Loss: 0.1173\n",
      "Epoch [1/10], Step [19000/36006], Loss: 0.0248\n",
      "Epoch [1/10], Step [19100/36006], Loss: 1.3278\n",
      "Epoch [1/10], Step [19200/36006], Loss: 0.2479\n",
      "Epoch [1/10], Step [19300/36006], Loss: 0.3688\n",
      "Epoch [1/10], Step [19400/36006], Loss: 0.1376\n",
      "Epoch [1/10], Step [19500/36006], Loss: 0.3228\n",
      "Epoch [1/10], Step [19600/36006], Loss: 0.1265\n",
      "Epoch [1/10], Step [19700/36006], Loss: 0.4053\n",
      "Epoch [1/10], Step [19800/36006], Loss: 0.8969\n",
      "Epoch [1/10], Step [19900/36006], Loss: 0.6833\n",
      "Epoch [1/10], Step [20000/36006], Loss: 0.5203\n",
      "Epoch [1/10], Step [20100/36006], Loss: 0.5576\n",
      "Epoch [1/10], Step [20200/36006], Loss: 0.0928\n",
      "Epoch [1/10], Step [20300/36006], Loss: 0.4497\n",
      "Epoch [1/10], Step [20400/36006], Loss: 0.1113\n",
      "Epoch [1/10], Step [20500/36006], Loss: 0.5513\n",
      "Epoch [1/10], Step [20600/36006], Loss: 1.1962\n",
      "Epoch [1/10], Step [20700/36006], Loss: 0.9349\n",
      "Epoch [1/10], Step [20800/36006], Loss: 0.1524\n",
      "Epoch [1/10], Step [20900/36006], Loss: 0.4259\n",
      "Epoch [1/10], Step [21000/36006], Loss: 0.0538\n",
      "Epoch [1/10], Step [21100/36006], Loss: 0.0259\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/stathis/git/Projects-YS19/ergasia3/project3_AI2.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/stathis/git/Projects-YS19/ergasia3/project3_AI2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/stathis/git/Projects-YS19/ergasia3/project3_AI2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/stathis/git/Projects-YS19/ergasia3/project3_AI2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/stathis/git/Projects-YS19/ergasia3/project3_AI2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/stathis/git/Projects-YS19/ergasia3/project3_AI2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Step [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/stathis/git/Projects-YS19/ergasia3/project3_AI2.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m            \u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, num_epochs, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, total_step, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/miniconda3/envs/AI2/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/AI2/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/AI2/lib/python3.9/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/AI2/lib/python3.9/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/AI2/lib/python3.9/site-packages/torch/optim/adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    363\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[1;32m    367\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create an sentiment classifier using bidirectional stacked RNNs with LSTM/GRU cells. Use the Adam optimizer and the cross-entropy loss function.\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "input_size = 200\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SentimentClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(X_train)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (reviews, labels) in enumerate(zip(X_train, y_train)):\n",
    "        reviews = reviews.reshape(1, 1, -1).to(device)\n",
    "        labels = labels.reshape(1, 1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(reviews)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    with torch.no_grad():\n",
    "    Y_train_pred = model(X_train);\n",
    "    loss_train = loss_func(Y_train_pred, y_train.unsqueeze(-1))/Y_train_pred.shape[0]\n",
    "    train_loss.append(loss_train.item());\n",
    "    Y_val_pred = model(X_validation);\n",
    "    loss_val = loss_func(Y_val_pred, y_validation.unsqueeze(-1))/Y_val_pred.shape[0]\n",
    "    val_loss.append(loss_val.item());\n",
    "\n",
    "    print(\"Epoch: \", epoch, \"Step: \", i, \"Loss: \", loss.item())\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for reviews, labels in zip(X_test, y_test):\n",
    "        reviews = reviews.reshape(1, 1, -1).to(device)\n",
    "        labels = labels.reshape(1, 1).to(device)\n",
    "        outputs = model(reviews)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test reviews: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Find the precision, recall and F1 score\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for reviews, labels in zip(X_test, y_test):\n",
    "        reviews = reviews.reshape(1, 1, -1).to(device)\n",
    "        labels = labels.reshape(1, 1).to(device)\n",
    "        outputs = model(reviews)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        y_true.append(labels.item())\n",
    "        y_pred.append(predicted.item())\n",
    "\n",
    "    print(\"Precision: \", precision_score(y_true, y_pred))\n",
    "    print(\"Recall: \", recall_score(y_true, y_pred))\n",
    "    print(\"F1 score: \", f1_score(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8692598595353863\n",
      "Recall:  0.7020069808027923\n",
      "F1 score:  0.7767318368332126\n"
     ]
    }
   ],
   "source": [
    "# Find the precision, recall and F1 score\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for reviews, labels in zip(X_test, y_test):\n",
    "        reviews = reviews.reshape(1, 1, -1).to(device)\n",
    "        labels = labels.reshape(1, 1).to(device)\n",
    "        outputs = model(reviews)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        y_true.append(labels.item())\n",
    "        y_pred.append(predicted.item())\n",
    "\n",
    "    print(\"Precision: \", precision_score(y_true, y_pred))\n",
    "    print(\"Recall: \", recall_score(y_true, y_pred))\n",
    "    print(\"F1 score: \", f1_score(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14287eb3e8c3f5c25b3ced007a168c47b9e249ff502873da572da074e7fa636"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
